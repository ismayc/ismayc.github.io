---
title: "MATH 141"
author: "Chester Ismay"
output: 
 ioslides_presentation:
   incremental: true
   keep_md: yes
   logo: ../figs/griffin.png
   widescreen: yes
   html_preview: false
subtitle: Logistic Regression
---

```{r setup, include=FALSE}
library(knitr)
library(rglwidget)
options(digits=3, width=100)
knitr::opts_chunk$set(echo = TRUE, fig.align = "center")
library(rgl)
knit_hooks$set(webgl = hook_webgl)
library(dplyr)
library(ggplot2)
library(oilabs)
library(openintro)
library(plotly)
```

```{r getdata, echo = FALSE, message=FALSE}
nyc <- read.csv("http://andrewpbray.github.io/data/nyc.csv")
```



## Review:  The geometry of regression models {.build}

- When you have two continuous predictors $x_1$, $x_2$, then your mean function
is 

*a plane*.

- When you have two continuous predictors $x_1$, $x_2$, and a categorical 
predictor $x_3$, then your mean function represents 

*parallel planes*.

- When you add in interaction effects, the planes become 

*tilted*.

## Model 2: Food + Decor + East {.smaller}

```{r}
m2 <- lm(Price ~ Food + Decor + East, data = nyc)
summary(m2)
```


## Model 3: Food + Decor + East + Decor:East {.smaller}

```{r}
m3 <- lm(Price ~ Food + Decor + East + Decor:East, data = nyc)
summary(m3)
```


## 3D plot

```{r echo=FALSE, eval=TRUE, webgl=TRUE}
colvec <- rep("steelblue", dim(nyc)[1])
colvec[nyc$East == 1] <- "orange"
plot3d(x = nyc$Food, y = nyc$Decor, z = nyc$Price, col = colvec, 
       xlab = "Food rating", ylab = "Decor Rating", zlab = "Price")
coefs <- m3$coef
rgl.viewpoint(zoom = .7)
planes3d(coefs["Food"], coefs["Decor"], -1, coefs["(Intercept)"],
         alpha = 0.4, col = "steelblue")
planes3d(coefs["Food"], coefs["Decor"] + coefs["Decor:East"], -1, 
         coefs["(Intercept)"] + coefs["East"], alpha = 0.4, col = "orange")
```


## Comparing Models

- The `East` term was significant in model 2, suggesting that there is a 
significant relationship between location and price.
- That term became non-significant when we allowed the slope of `Decor` to vary
with location, and that difference in slopes was also non-significant.
- Notice that the slope estimate for a given variable will almost *always* change 
depending on the other variables that are in the model.



# Logistic Regression

## Building a spam filter {.build .smaller}

```{r}
library(openintro)
head(email)
# where did this data come from / how was it collected?
```


## How was the data collected? {.flexbox .vcenter .build}

<img src="../figs/reed-email.png" width="800px" />

1. Choose a single email account
2. Save each email that comes in during a given time frame
3. Create dummy variables for each text component of interest
4. Visually classify each as spam or not


## Simple Filter A {.build}

Predicting spam or not using the presence of "winner"

```{r echo = FALSE, fig.width=6, fig.height=4}
qplot(x = winner, fill = factor(spam), data = email,
       ylab = "proportion", geom = "bar")
```

If "winner" then "spam"?


## Simple Filter B {.build}

Predicting spam or not using number of characters (in thousands)

```{r echo = FALSE, fig.width=6, fig.height = 4.5}
qplot(x = num_char, col = factor(spam), data = email, geom = "density")
```


## Simple Filter B {.build}

Predicting spam or not using log number of characters (in thousands)

```{r echo = FALSE, fig.width=6, fig.height = 4}
qplot(x = log(num_char), col = factor(spam), data = email, geom = "density")
```

If `log(num_char)` < 1, then "spam"?


## Each simple filter can be thought of as a regression model. {.build .smaller}

Remember our convention for Modeling Variable Types:

- $K$: categorical variable with 2 groups
- $G$: categorical variable with 3+ groups
- $H$: continuous variable

### Filter A
$spam \sim winner; \quad K_1 \sim K_2$

### Filter B
$spam \sim log(num\_char); \quad K \sim H$


Each one by itself has poor predictive power, so how can we combine them into
a single stronger model?

## {.flexbox .vcenter .build}

<img src="../figs/good-bad.jpg" width="800px" />

