---
title: "MATH 141"
author: "Chester Ismay"
output:
 ioslides_presentation:
   incremental: true
   keep_md: yes
   logo: ../figs/griffin.png
   widescreen: yes
subtitle: Simple Linear Regression II
---

```{r setup, include=FALSE}
library(knitr)
options(digits=3, width = 100)
knitr::opts_chunk$set(echo = TRUE, fig.align = "center")
library(dplyr)
library(ggplot2)
library(oilabs)
library(openintro)
library(plotly)
poverty <- read.delim("poverty.txt", header = TRUE) %>% select(-Percent)
```


## Estimation in R {.smaller .build}

```{r fitlm}
m1 <- lm(Graduates ~ Poverty, data = poverty)
summary(m1)
```


## The `lm` object {.build .smaller}

```{r showlm}
attributes(m1)
m1$fit

```

## The `lm` object {.build .smaller}

```{r showlm2}
m1$coef
m1$residuals
```


## Interpretation of $b_1$ {.build}

The **slope** describes the estimated difference in the $y$ variable if the explanatory
variable $x$ for a case happened to be one unit larger.

```{r}
m1$coef[2]
```

*For each additional percentage point of people living below the poverty level,
we expect a state to have a proportion of high school graduates that is 0.898
lower*.

**Be Cautious**: if it is observational data, you do not have evidence of a 
*causal link*, but of an association, which still can be used for prediction.


## Interpretation of $b_0$ {.build}

The **intercept** is the estimated $y$ value that will be taken by a case with 
an $x$ value of zero.

```{r}
m1$coef[1]
```

While necessary for prediction, the intercept often has no meaningful interpretation.

## Inference for Regression {.build}
We can fit a line through any cloud of points that we please, but if we
just have a *sample* of data, any trend we detect doesn't necessarily 
demonstrate that the trend exists in the *population* at large.

<!--
## Plato's Allegory of the Cave

<div class="centered">
![](http://4.bp.blogspot.com/-rV1c4Xh4SSE/UZshhTTdFsI/AAAAAAAACQA/1VkmOaF7WFU/s1600/plato-cave.jpg)
</div>
-->

## Statistical Inference {.build}

**Goal**: use *statistics* calculated from data to makes inferences about the 
nature of *parameters*.

In regression,

- parameters: $\beta_0$, $\beta_1$
- statistics: $b_0$, $b_1$

Classical tools of inference:

- Confidence Intervals
- Hypothesis Tests


## Unemployment and elections {.build}
```{r echo = FALSE}
library(openintro)
data(unempl)
data(house)
data(president); pres <- president
year   <- seq(1898, 2010, 4)+1
n      <- length(year)
unemp  <- rep(0, n)
change <- rep(0, n)
presid <- rep("", n)
party  <- rep("", n)
for(i in 1:n){
	urow <- which(unempl$year == year[i]-1)
	if(i < n){
		prow <- which(pres$end > year[i])[1]
	} else {
		prow <- which(pres$potus == "Barack Obama")
	}
	hrow <- which(house$yearEnd >= year[i])[1]
	party[i] <- as.character(pres$party[prow])
	if(substr(house$p1[hrow],1,5) == substr(party[i],1,5)){
		oldHouse <- house$np1[hrow] / house$seats[hrow]
	} else {
		oldHouse <- house$np2[hrow] / house$seats[hrow]
	}
	if(substr(house$p1[hrow+1],1,5) == substr(party[i],1,5)){
		newHouse <- house$np1[hrow+1] / house$seats[hrow+1]
	} else {
		newHouse <- house$np2[hrow+1] / house$seats[hrow+1]
	}
	change[i] <- (newHouse - oldHouse)/oldHouse * 100
	presid[i] <- as.character(pres$potus[prow])
	unemp[i]  <- unempl$unemp[urow]
}

unemployPres <- data.frame(year=year, potus=presid, party=party, unemp=unemp, change=change)
unemployPres[29, 3] <- "Democratic"
```

```{r echo=FALSE, fig.width=7}
levels(unemployPres$party) <- levels(unemployPres$party)[c(1, 3, 2)]
levels(unemployPres$party)[2:3] <- c("Rep", "Dem")
qplot(x = unemp, y = change, col = party, data = unemployPres)
```

<center>
**Reigning theory**: voters will punish candidates from the President's party
at the ballot box when unemployment is high.
</center>

## Unemployment and elections

```{r echo=FALSE, fig.width=7}
m1 <- lm(change ~ unemp, data = unemployPres)
plot1 <- qplot(x = unemp, y = change, col = party, data = unemployPres) +
  geom_abline(intercept = m1$coef[1], slope = m1$coef[2])
ggplotly(plot1)
```

<center>
**Reigning theory**: voters will punish candidates from the President's party
at the ballot box when unemployment is high.
</center>

## Unemployment and elections {.build}

```{r echo = FALSE}
library(dplyr)
ump <- filter(unemployPres, unemp < 15)
m0 <- lm(change ~ unemp, data = ump)
qplot(x = unemp, y = change, col = party, data = ump) +
  geom_abline(intercept = m0$coef[1], slope = m0$coef[2])
```

<center>
Some evidence of a negative linear relationship between unemployment level
and change in party support - or is there?
</center>

## Hypothesis Test for Regression Slope {.build}

$H_0:$ There is no relationship between unemployment level and change in 
party support.

$H_O: \beta_1 = 0$

### Method
If there is no relationship, the pairing between $X$ and $Y$ is
artificial and we can randomize:

1. Create synthetic data sets under $H_0$ by shuffling $X$.
2. Compute a new regression line for each data set and store each $b_1$.
3. See where your observed $b_1$ falls in the distribution of $b_1$'s under $H_0$.


##

```{r echo = FALSE}
set.seed(764)
ump_shuffled <- ump
```

```{r}
ump_shuffled$unemp <- sample(ump_shuffled$unemp)
qplot(x = unemp, y = change, col = party, data = ump_shuffled)
```

## First $b_1$

```{r echo = FALSE}
m1 <- lm(change ~ unemp, data = ump_shuffled)
qplot(x = unemp, y = change, col = party, data = ump_shuffled) +
  geom_abline(intercept = m1$coef[1], slope = m1$coef[2])
```


## Second $b_1$

```{r, echo = FALSE}
ump_shuffled$unemp <- sample(ump$unemp)
m1 <- lm(change ~ unemp, data = ump_shuffled)
qplot(x = unemp, y = change, col = party, data = ump) +
  geom_abline(intercept = m1$coef[1], slope = m1$coef[2])
```


## 100 $b_1$'s

```{r echo = FALSE, cache=TRUE}
line_df <- data.frame(matrix(rep(0, 200), ncol = 2))
for (i in 1:100) {
  ump_shuffled$unemp <- sample(ump$unemp)
  m1 <- lm(change ~ unemp, data = ump_shuffled)
  line_df[i, ] <- c(m1$coef)
}

p <- qplot(x = unemp, y = change, col = party, data = ump)

for (i in 1:100) {
  p <- p + geom_abline(intercept = line_df[i, 1], slope = line_df[i, 2],
                       alpha = .3)
}

p
```


## Sampling dist. of $b_1$

```{r echo = FALSE, message = FALSE}
qplot(line_df[, 2], geom = "density") + 
  geom_vline(xintercept = m0$coef[2], col = "tomato") +
  xlab("Simulated Sample Slope Coefficients")
```


## Hypothesis Test for Regression Slope {.smaller}

```{r}
m0 <- lm(change ~ unemp, data = ump)
summary(m0)
```


## Hypothesis Test for Regression Slope {.smaller .build}

- Each line in the summary table is a hypothesis test that the parameter is zero.
- Under certain conditions, the test statistic associated with $b$'s is distributed 
like $t$ random variables with $n - p$ degrees of freedom.

$$ \frac{b - \beta}{SE} \sim t_{df = n - p}$$

```{r}
t_stat <- (-1.0010 - 0)/0.8717
pt(t_stat, df = 27 - 2) * 2
```


## Conditions for inference

1. **Linearity**: linear trend between $X$ and $Y$, check with residual plot.
2. **Independent errors**: check with residual plot for serial correlation.
3. **Normally distributed errors**: check for linearity in qq-plot.
4. **Constant variance**: look for constant spread in residual plot.

