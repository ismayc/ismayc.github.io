---
title: "MATH 141"
author: "Chester Ismay"
output:
 ioslides_presentation:
#   incremental: true
   keep_md: yes
   logo: ../figs/griffin.png
   widescreen: yes
subtitle: Simple Linear Regression III
---

<style type="text/css">
    ul { list-style-type: upper-alpha; }
</style>

```{r setup, include=FALSE}
library(knitr)
options(digits=3)
knitr::opts_chunk$set(echo = TRUE, fig.align = "center")
library(dplyr)
library(ggplot2)
library(oilabs)
library(openintro)
library(plotly)
poverty <- read.delim("poverty.txt", header = TRUE) %>% select(-Percent)
```

# Plicker time!

##

In a study of legibility and visibility of highway signs, a Pennsylvania research firm determined the maximum distance (in feet) at which each of 15 drivers could read a newly designed highway sign. The firm also recorded the age (in years) of each of the drivers. The 15 drivers in the study ranged from 18 to 82 years old. The firm wanted to investigate if there was an association between a driver's age and the maximum distance from which they could read the new style of sign. They believed that younger drivers could read the sign from a farther distance than older drivers.

##

<!--<div class="columns-2">-->
```{r echo=FALSE, fig.height=2.5}
download.file("http://stat.duke.edu/~mc301/data/vision.csv", destfile = "/Users/cismay/Google Drive/ismayc.github.io/slides/11/vision.csv")
vision <- read.csv("/Users/cismay/Google Drive/ismayc.github.io/slides/11/vision.csv")
qplot(x = age, y = distance, data = vision)
```


<!--
<center>
<img src="../figs/highwayscatterplot.png" width = 350>
</center>
-->

A scatterplot of the data is shown. Which of the following could be the value of the correlation coefficient for this study?
<!--</div>-->


- $R = 0.7$
- $R = 0$
- $R = -0.3$
- $R = -0.9$

## 

Assuming conditions are met, the least squares regression equation for predicting reading distance (in feet) from age (in years) is $\widehat{distance} = 561.40 - 2.86\,age$. Which is the best (most complete) interpretation of the slope of this line?

- A newborn could read the sign from about 561 feet away.
- For each additional year in age, the predicted maximal reading distance decreases by 2.86 feet on average.
- As the age of drivers increases from 18 to 82 years old, the reading distance drops by 286%.
-  Each increase of one year in age is associated with an average predicted decrease in reading distance of 2.86%.

## Type of Problem


Researchers Oken et al. conducted a study where 135 generally
healthy men and women aged 65-85 years were randomly assigned to either 6 months of Hatha yoga class (44 people), walking exercise (47 people), or wait-list control (44 people). One of the outcomes of interest was change in "chair sit and reach‚Äù - a measure of how far the subject can reach out while sitting on a
chair, without losing balance.  Does an association exist between the reach measurement and the exercise assignment?

- Multiple means
- Multiple proportions
- Correlation/regression
- None of the above


## Type of Problem

In an article published in the Lancet (2001), researchers shared their findings from a study where they followed 6272 Swedish men for 30 years to see whether there was an association between the amount of fish in the diet and likelihood of prostate cancer.  The amount of fish was grouped into categories of large, moderate, small, and none.  It was noted whether the men obtained prostate cancer or not.

- Multiple means
- Multiple proportions
- Correlation/regression
- None of the above

## Type of Problem

Is hand span a good predictor of how much candy you can grab? Using 45 college students as subjects, researchers set out to explore whether a positive association exists between hand span (cm) and the number of tootsie rolls each subject could grab.

- Multiple means
- Multiple proportions
- Correlation/regression
- None of the above

## Conditions for inference with simple linear regression

1. **Linearity**: linear trend between $X$ and $Y$, check with residual plot.
2. **Independent errors**: check with residual plot for serial correlation.
3. **Normally distributed errors**: check for linearity in qq-plot.
4. **Constant variance**: look for constant spread in residual plot.


##

<center>
<img src="../figs/reg_conditions.png" width = 950>
</center>

# Outliers

## What is an outlier? {.build}

<div class="columns-2">
![](http://marcoghislanzoni.com/blog/wp-content/uploads/2013/10/outliers_gladwell.jpg)

**Outlier** is a general term to describe a data point that doesn't follow the
pattern set by the bulk of the data, when one takes into account the model.
</div>


## Outlier Example One

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# this chunk sets the chunk options for the whole document
require(knitr)
opts_chunk$set(warning=FALSE, message=FALSE)
```

```{r, echo=FALSE}
library(openintro)
COL <- c('#55000088','#225588')
set.seed(238)
n <- c(50, 25, 78, 55, 70, 150)
m <- c(12, -4, 7, -19, 0, 40)
xr <- list(0.3, c(2), 1.42, runif(4,1.45,1.55), 5.78, -0.6)
yr <- list(-4, c(-8), 19, c(-17,-20,-21,-19), 12, -23.2)
i <- 1
x <- runif(n[i])
y <- m[i]*x + rnorm(n[i])
x <- c(x,xr[[i]])
y <- c(y,yr[[i]])
par(mar=c(4,4,1,1), las=1, mgp=c(2.5,0.5,0), cex.lab = 1.25, cex.axis = 1.25, mfrow = c(2,1))
lmPlot(x, y, col = COL[2], lCol = COL[1], lwd = 3)
```


## Outlier Example Two

```{r, echo=FALSE}
i <- 2
x <- runif(n[i])
y <- m[i]*x + rnorm(n[i])
x <- c(x,xr[[i]])
y <- c(y,yr[[i]])
par(mar=c(4,4,1,1), las=1, mgp=c(2.5,0.5,0), cex.lab = 1.25, cex.axis = 1.25, mfrow = c(2,1))
lmPlot(x, y, col = COL[2], lCol = COL[1], lwd = 3)
```


## Outlier Example Three

```{r, echo=FALSE}
i <- 3
x <- runif(n[i])
y <- m[i]*x + rnorm(n[i])
x <- c(x,xr[[i]])
y <- c(y,yr[[i]])
par(mar=c(4,4,1,1), las=1, mgp=c(2.5,0.5,0), cex.lab = 1.25, cex.axis = 1.25, mfrow = c(2,1))
lmPlot(x, y, col = COL[2], lCol = COL[1], lwd = 3)
```


## Outlier Example Four

```{r, echo=FALSE}
i <- 5
x <- runif(n[i])
y <- m[i]*x + rnorm(n[i])
x <- c(x,xr[[i]])
y <- c(y,yr[[i]])
par(mar=c(4,4,1,1), las=1, mgp=c(2.5,0.5,0), cex.lab = 1.25, cex.axis = 1.25, mfrow = c(2,1))
lmPlot(x, y, col = COL[2], lCol = COL[1], lwd = 3)
```


## Outlier Example Four (modified)

```{r, echo=FALSE}
par(mar=c(4,4,1,1), las=1, mgp=c(2.5,0.5,0), cex.lab = 1.25, cex.axis = 1.25, mfrow = c(2,1))
lmPlot(x[1:70], y[1:70], col = COL[2], lCol = COL[1], lwd = 3, xlim = range(x), ylim = range(y))
```

## Outliers, leverage, influence {.build}

**Outliers** are points that don't fit the trend in the rest of the data.

**High leverage points** have the potential to have an unusually large influence 
on the fitted model.

**Influential points** are high leverage points that cause a very different
line to be fit than would be with that point removed.


## Example of high leverage, high influence

We have data on the surface temperature and light intensity of 47 stars in the
star cluster CYG OB1, near Cygnus.

```{r, echo=FALSE}
library(faraway)
data(star)
par(mar=c(4,4,2,1), las=1, mgp=c(2.5,0.7,0), cex.lab = 1.25, cex.axis = 1.25)
plot(light ~ temp, data = star, pch=19, col=COL[2], xlab = "log(temp)", ylab = "log(light intensity)")
```


## Example of high leverage, high influence

We have data on the surface temperature and light intensity of 47 stars in the
star cluster CYG OB1, near Cygnus.

```{r, echo=FALSE}
par(mar=c(4,4,2,1), las=1, mgp=c(2.5,0.7,0), cex.lab = 1.25, cex.axis = 1.25)
plot(light ~ temp, data = star, pch=19, col=COL[2], xlab = "log(temp)", ylab = "log(light intensity)")
abline(lm(light~temp, data = star), col = "darkgreen", lwd = 3, lty = 2)
legend("top", inset = 0.05, "w/ outliers", lty = 2, lwd = 2, col = "darkgreen")
```


## Example of high leverage, high influence

We have data on the surface temperature and light intensity of 47 stars in the
star cluster CYG OB1, near Cygnus.

```{r, echo=FALSE}
par(mar=c(4,4,2,1), las=1, mgp=c(2.5,0.7,0), cex.lab = 1.25, cex.axis = 1.25)
plot(light ~ temp, data = star, pch=19, col=COL[2], xlab = "log(temp)", ylab = "log(light intensity)")
abline(lm(light~temp, data = star), col = "darkgreen", lwd = 3, lty = 2)
abline(lm(light[temp>4]~temp[temp>4], data = star), col = COL[1], lwd = 3)
legend("top", inset = 0.05, c("w/ outliers","w/o outliers"), lty = c(2,1), lwd = c(2,3), col = c("darkgreen",COL[1]))
```


## Example of high leverage, low influence

```{r, echo=FALSE}
set.seed(12)
i <- 2
x <- runif(n[i])
y <- m[i]*x + rnorm(n[i])
x <- c(x,xr[[i]])
y <- c(y,yr[[i]])
y <- y - mean(y)
par(mar=c(4,4,1,1), las=1, mgp=c(2.5,0.5,0), cex.lab = 1.25, cex.axis = 1.25, mfrow = c(2,1))
lmPlot(x, y, col = COL[2], lCol = COL[1], lwd = 3)
```

<!--
## From leverage to influence

**Leverage** measures the weight given to each point in determining the regression
line.

**Influence** measures how different the regression line would be without a given
point. Often measured with *Cook's Distance*.

```{r, echo=FALSE, fig.height=4, }
i <- 5
x <- runif(n[i])
y <- m[i]*x + rnorm(n[i])
x <- c(x,xr[[i]])
y <- c(y,yr[[i]])
par(mfrow = c(1, 2))
plot(x, y, col = COL[2], pch = 16)
abline(lm(y ~ x), col = COL[1], lwd = 3)
x2 <- x[1:70]
y2 <- y[1:70]
plot(x2, y2, col = COL[2], pch = 16, xlim = range(x), ylim = range(y))
abline(lm(y2 ~ x2), col = COL[1], lwd = 3)
```


## {.smaller}

In the following plots are there outliers, leverage pts, or influential pts?

<center>
<img src="../figs/outliers.png" height = 550>
</center>


# Some chatter from the internets

## 2016 Election {.build}
<center>
<img src="../figs/538.png" width = 850>
</center>

**Question at hand**: How will Obama's 46% approval rating effect his
party's candidate for the 2016 presidential election?


## 
<center>
<img src="../figs/538-1.png" width = 850>
</center>


##  {.build}
<center>
<img src="../figs/538-2.png" width = 850>
</center>

</br>
</br>

### How would you visualize this data?

##  {.build}
<center>
<img src="../figs/538-3.png" width = 850>
</center>

</br>

### Why is it ridiculous?

-->
